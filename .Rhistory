ppred=mean(p.hat),
count=n()) %>%
mutate(se.fit=sqrt( ppred*(1-ppred)/count ) )
hl_stat <- with(hl_df,
sum( (y-count*ppred)^2 / (count*ppred*(1-ppred)) )
)
p.value <- 1-pchisq(hl_stat, nrow(hl_df)-2)
gr <- ggplot(hl_df,aes(x=ppred,
y=y/count,
ymin=y/count-2*se.fit,
ymax=y/count+2*se.fit))+
geom_point()+
geom_linerange(color=grey(0.75))+
geom_abline(intercept=0,slope=1)+
xlab("Predicted Probability")+
ylab("Observed Proportion")+
theme_minimal()+
annotate("text", x = 0.2, y = 0.8,
label = paste0("p-value = ", round(p.value,3)),
size = 4, color = "blue")
out <- list(hl_stat = hl_stat,
deg.freed = nrow(hl_df)-2,
p.value = p.value,
min.count.in.bin = min(hl_df$count),
max.count.in.bin = max(hl_df$count),
mean.count.in.bin = mean(hl_df$count),
graph = gr)
return(out)
}
binning_bernoulli(m1_logistic,150)$graph
hosmer_lemeshow(m1_logistic,150)$graph
with(
df,
table(loan_status,previous_loan_defaults_on_file)
)
influenceIndexPlot(m1_logistic)
residualPlots(m1_logistic, type = "rstudent", plot = F)
m2_logistic <- glm(loan_status ~ age + sex + education +
income + employment_experience +
home_ownership +
poly(loan_amount,2) + loan_intent + poly(loan_int_rate,2) +
poly(loan_percent_income,2) + credit_history_length +
poly(credit_score,2) + previous_loan_defaults_on_file +
education:employment_experience +
(employment_experience +
credit_score):home_ownership,
data = train_df,
family = binomial)
binning_bernoulli(m2_logistic,150)$graph
hosmer_lemeshow(m2_logistic,150)$graph
m0_logistic <- glm(loan_status ~ 1,
family = binomial,train_df)
m1_logistic_aic1 <- step(m0_logistic,
scope=list(lower=m0_logistic, upper=m2_logistic),
direction = "both", trace = F)
m1_logistic_aic2 <- step(m2_logistic,
scope=list(lower=m0_logistic, upper=m2_logistic),
direction = "both", trace = F)
m1_logistic_aic1$aic
m1_logistic_aic2$aic
m1_logistic_aic1$formula
m1_logistic_aic2$formula
binning_bernoulli(m1_logistic_aic1,150)$graph
hosmer_lemeshow(m1_logistic_aic1,150)$graph
# Pseudo R^2
PseudoR2(m1_logistic_aic1, which = "Nagelkerke")
# Contingency table
# (for now use a basic threshold of 0.5)
preds.logistic <- as.numeric(m1_logistic_aic1$fitted.values >= .5)
(cont.table.logistic <- table(train_df$loan_status, preds.logistic))
(sum(diag(cont.table.logistic))) / sum(cont.table.logistic)
x_train <- model.matrix(m2_logistic)[,-1] # no intercept
y_train <- train_df$loan_status
n_folds <- 10 # in this case each fold contains 33,750 / 10 = 3,375 observations
set.seed(123)
# Ridge
# Use cross-validation to find the best lambda (tuning parameter)
cv.m1_ridge <- cv.glmnet(x_train, y_train,
alpha = 0, # 0 - Ridge, 1 - Lasso
nfolds = n_folds,
family = binomial)
m1_ridge <- glmnet(x_train, y_train,
alpha = 0, # 0 - Ridge, 1 - Lasso
lambda = cv.m1_ridge$lambda.min,
family = binomial)
# Lasso
cv.m1_lasso <- cv.glmnet(x_train, y_train,
alpha = 1, # 0 - Ridge, 1 - Lasso
nfolds = n_folds,
family = binomial)
m1_lasso <- glmnet(x_train, y_train,
alpha = 1, # 0 - Ridge, 1 - Lasso
lambda = cv.m1_lasso$lambda.min,
family = binomial)
cv.m1_ridge$lambda.min # for Ridge
cv.m1_lasso$lambda.min # for Lasso
m <- cbind(
round(coef(m1_ridge),3),
round(ifelse(coef(m1_lasso)==0,NA,coef(m1_lasso)),3) # put NAs for 0s
)
colnames(m) <- c("ridge","lasso")
m <- m %>%
as.matrix(.) %>%
as.data.frame(.)
m <- rownames_to_column(m, "predictor")
m1 <- as.data.frame(round(coef(m1_logistic_aic1),3))
colnames(m1) <- "aic_step"
m1 <- rownames_to_column(m1, "predictor")
full_join(m1,m,by = "predictor")
preds_ridge <- as.numeric(predict(m1_ridge,
s = m1_ridge$lambda,
newx = x_train, type = "response") >= 0.5)
preds_lasso <- as.numeric(predict(m1_lasso,
s = m1_lasso$lambda,
newx = x_train, type = "response") >= 0.5)
(contig_ridge <- xtabs( ~ preds_ridge + train_df$loan_status))
(contig_lasso <- xtabs( ~ preds_lasso + train_df$loan_status))
sum(diag(contig_ridge))/ nrow(train_df)
sum(diag(contig_lasso))/ nrow(train_df)
boxM <- boxM( cbind(age, sex, education, income, employment_experience,
home_ownership, loan_amount,loan_intent, loan_int_rate,
loan_percent_income, credit_history_length,
credit_score,previous_loan_defaults_on_file) ~ loan_status,
train_df)
boxM
set.seed(12345)
lda_m_select <- stepclass(
loan_status ~ age + sex + education + income +
employment_experience + home_ownership +
loan_amount + loan_intent + loan_int_rate +
loan_percent_income + credit_history_length +
credit_score + previous_loan_defaults_on_file,
data = train_df,
method = "lda",
direction = "both",
criterion = "AS", # ability to separate (aka ability to classify correctly)
improvement = .0001, # min improvement
cv.groups = rep(sample(1:10),3375) # 10 folds
)
plot(lda_m_select)
lda_m_select$result.pm
m_lda_best <- lda(loan_status ~ loan_percent_income +
loan_int_rate + loan_amount + income,
train_df)
m_lda_best
plot(m_lda_best)
(preds_lda <- xtabs( ~ predict(m_lda_best)$class + train_df$loan_status))
sum(diag(preds_lda))/ nrow(train_df)
set.seed(12345)
qda_m_select <- stepclass(
loan_status ~ age + sex + education + income +
employment_experience + home_ownership +
loan_amount + loan_intent + loan_int_rate +
loan_percent_income + credit_history_length +
credit_score,
data = train_df,
method = "qda",
direction = "both",
criterion = "AS", # ability to separate (aka ability to classify correctly)
improvement = .0001, # min improvement
cv.groups = rep(sample(1:10),3375) # 10 folds
)
plot(qda_m_select)
qda_m_select$result.pm
m_qda_best <- qda(loan_status ~ loan_percent_income +
loan_amount + loan_int_rate +
credit_history_length,
train_df)
m_qda_best
(preds_qda <- xtabs( ~ predict(m_qda_best)$class + train_df$loan_status))
sum(diag(preds_qda))/ nrow(train_df)
# training data set
train_df$loan_percent_income_sq <- train_df$loan_percent_income^2
train_df$loan_int_rate_sq <- train_df$loan_int_rate^2
train_df$loan_amount_sq <- train_df$loan_amount^2
train_df$credit_score_sq <- train_df$credit_score^2
# test data set (we'll use it later)
test_df$loan_percent_income_sq <- test_df$loan_percent_income^2
test_df$loan_int_rate_sq <- test_df$loan_int_rate^2
test_df$loan_amount_sq <- test_df$loan_amount^2
test_df$credit_score_sq <- test_df$credit_score^2
set.seed(12345)
nb_m_select <- stepclass(
loan_status ~ loan_percent_income +
loan_percent_income_sq +
loan_int_rate + loan_int_rate_sq +
loan_amount + loan_amount_sq +
home_ownership + loan_intent +
credit_score + credit_score_sq +
income + employment_experience + age,
data = train_df,
method = "NaiveBayes",
direction = "both",
criterion = "AS", # ability to separate (aka ability to classify correctly)
improvement = .0001, # min improvement
cv.groups = rep(sample(1:10),3375) # 10 folds
)
plot(nb_m_select)
nb_m_select$result.pm
m_nb_best <- naiveBayes(loan_status ~ loan_percent_income +
loan_percent_income_sq +
loan_amount + loan_amount_sq +
loan_int_rate + loan_int_rate_sq +
home_ownership + loan_intent,
train_df)
m_nb_best$tables
(preds_nb <- xtabs( ~ predict(m_nb_best, newdata = train_df) + train_df$loan_status))
sum(diag(preds_nb))/ nrow(train_df)
set.seed(123)
m1_RF <- randomForest(loan_status ~ age + sex + education +
income + employment_experience +
home_ownership + loan_amount + loan_intent +
loan_int_rate + loan_percent_income +
credit_history_length + credit_score +
previous_loan_defaults_on_file,
data = train_df,
importance = TRUE)
m1_RF
varImpPlot(m1_RF)
(preds_RF <- m1_RF$confusion[,-3])
sum(diag(preds_RF))/ nrow(train_df)
(preds_RF <- m1_RF$confusion[,-3])
sum(diag(preds_RF))/ nrow(train_df)
set.seed(123)
m1_boost <- gbm((as.numeric(loan_status)-1) ~ age + sex + education +
income + employment_experience +
home_ownership + loan_amount + loan_intent +
loan_int_rate + loan_percent_income +
credit_history_length + credit_score +
previous_loan_defaults_on_file,
data = train_df,
distribution = "bernoulli",
interaction.depth = 2)
summary(m1_boost)
preds_boost <- as.numeric(predict(m1_boost, type = "response") >= 0.5)
(contig_boost <- xtabs( ~ preds_boost + train_df$loan_status ))
sum(diag(contig_boost))/ nrow(train_df)
x_train <- model.matrix(m2_logistic)[,-1] # for Ridge and Lasso
# predictions from different models
all_models <- list(
# AIC based
m1_logistic_aic1$fitted.values,
# Ridge
as.vector(predict(m1_ridge,
s = m1_ridge$lambda,
type = "response",
newx = x_train)),
# Lasso
as.vector(predict(m1_lasso,
s = m1_lasso$lambda,
type = "response",
newx = x_train)),
# LDA
predict(m_lda_best)$posterior[,2],
# QDA
predict(m_qda_best)$posterior[,2],
# Naive Bayes
predict(m_nb_best,
newdata = train_df,
type = "raw")[,2],
# Random Forest
predict(m1_RF, type = "prob")[,2],
# Boosting
predict(m1_boost, type = "response")
)
models_names <- c("AIC", "Ridge",
"Lasso", "LDA",
"QDA", "NB","RF","Boost")
# store AUC parameters here
AUCs <- matrix(ncol = 5, nrow = length(all_models))
colnames(AUCs) <- c("threshold", "sensitivity", "specificity",
"AUC","model")
# Plots
for (i in 1:nrow(AUCs)){
roc_m <- roc(
response = train_df$loan_status,
predictor = all_models[[i]])
best_coords <- coords(roc_m,
x = "best",
best.method = "closest.topleft",
ret = c("threshold",
"sensitivity",
"specificity")) %>%
dplyr::mutate(AUC = as.numeric(roc_m$auc),
model = models_names[i])
AUCs[i,] <- as.vector(unlist(best_coords))
p <- ggroc(roc_m, legacy.axes = TRUE) +
labs(title = models_names[i]) +
annotate('text', x = .5, y = .5,
label = paste0('AUC: ', round(best_coords["AUC"], 3),
'\nBest threshold: ',
round(best_coords["threshold"], 3))) +
geom_vline(xintercept = as.numeric(1 - best_coords["specificity"]),
linetype = "dashed", color = "red") +
geom_hline(yintercept = as.numeric(best_coords["sensitivity"]),
linetype = "dashed", color = "red")
print(p)
}
# make it a data frame to refer to it later
AUCs <- as.data.frame(AUCs)
# Model matrix for test df (for Ridge and Lasso)
x_test <- model.matrix(loan_status ~ age + sex + education +
income + employment_experience +
home_ownership +
poly(loan_amount,2) + loan_intent + poly(loan_int_rate,2) +
poly(loan_percent_income,2) + credit_history_length +
poly(credit_score,2) + previous_loan_defaults_on_file +
education:employment_experience +
(employment_experience +
credit_score):home_ownership,
test_df)[,-1]
# AIC based
class.preds.AIC.logistic <- as.numeric(predict(m1_logistic_aic1,
newdata = test_df,
type = "response") > AUCs$threshold[1])
# Ridge
class.preds.Ridge <- as.numeric(as.vector(
predict(m1_ridge,
s = m1_ridge$lambda,
type = "response",
newx = x_test)) > AUCs$threshold[2])
# Lasso
class.preds.Lasso <- as.numeric(as.vector(
predict(m1_lasso,
s = m1_lasso$lambda,
type = "response",
newx = x_test)) > AUCs$threshold[3])
# LDA
class.preds.LDA <- as.numeric(as.vector(
predict(m_lda_best,
newdata = test_df)$posterior[,2]) > AUCs$threshold[4])
# QDA
class.preds.QDA <- as.numeric(as.vector(
predict(m_qda_best,
newdata = test_df)$posterior[,2]) > AUCs$threshold[5])
# Naive Bayes
class.preds.NB <- as.numeric(as.vector(
predict(m_nb_best,
newdata = test_df,
type = "raw")[,2]) > AUCs$threshold[6])
# Random Forest
class.preds.RF <- as.vector(as.numeric(predict(m1_RF,
newdata = test_df,
type = "prob")[,2] > AUCs$threshold[7]))
# Boosting
class.preds.Boost <- as.numeric(predict(m1_boost, #n.trees = best_n.trees,
newdata = test_df,
type = "response") > AUCs$threshold[8])
test.preds <- list(class.preds.AIC.logistic,
class.preds.Ridge,
class.preds.Lasso,
class.preds.LDA,
class.preds.QDA,
class.preds.NB,
class.preds.RF,
class.preds.Boost)
methods.preds <- c("AIC", "Ridge", "Lasso", "LDA", "QDA", "NB", "RF", "Boost")
for (i in 1:length(test.preds)){
cont.table <- xtabs( ~ test.preds[[i]] + test_df$loan_status)
prop.cor <- sum(diag(cont.table))/ nrow(test_df)
print(paste0(methods.preds[i],": ",round(prop.cor,3)))
print(cont.table)
}
(preds <- (table(test_df$loan_status) / nrow(test_df) ))
(rand.guess <- sum(preds^2))
set.seed(12345)
N_new_test_dfs <- 1000
small_test_df_size <- 250
preds_small_size <- matrix(ncol = length(test.preds)+1, nrow = N_new_test_dfs)
for (test_df_N in 1:N_new_test_dfs){
# random 250 observations
rand.rows <- sample(1:nrow(test_df), small_test_df_size)
resp <- test_df$loan_status
# for each model
for (i in 1:length(test.preds)){
cont.table <- xtabs( ~ test.preds[[i]][rand.rows] + resp[rand.rows])
prop.cor <- sum(diag(cont.table))/ small_test_df_size
preds_small_size[test_df_N,i] <- prop.cor
}
# for random guessing
rang.guess.preds <- sample(c(0,1), 250, replace = T, prob = preds)
cont.table <- xtabs( ~ rang.guess.preds + resp[rand.rows])
prop.cor <- sum(diag(cont.table))/ small_test_df_size
preds_small_size[test_df_N,length(test.preds)+1] <- prop.cor
}
# Visualization
preds_small_size_df_wide <- as.data.frame(preds_small_size)
colnames(preds_small_size_df_wide) <- c(methods.preds, "Random")
preds_small_size_df <- preds_small_size_df_wide %>%
pivot_longer(cols = everything(),
names_to = "Method",values_to = "Pred.rate")
preds_small_size_df$Method <- as.factor(preds_small_size_df$Method)
vline_df <- data.frame(
Method = c(methods.preds, "Random"),
Mean = colMeans(preds_small_size),
y = c(250,270,250,270,240,210,240,210,250)
)
ggplot(preds_small_size_df, aes(x=Pred.rate,
fill = Method))+
geom_histogram(position = "identity", alpha = .4,bins = 75, color="black")+
geom_vline(data = vline_df, aes(xintercept = Mean, color = Method), linewidth = 1.5) +
geom_text(data = vline_df, aes(x = Mean, y = y, label = Method),
color = "black", angle = 90, vjust = -0.5, size = 4)+
scale_x_continuous(breaks = round(c(vline_df$Mean,
rand.guess),2))+
labs(title = paste0("Predictions for ",N_new_test_dfs,
" test data sets with ",small_test_df_size,
" observations in each"),
x = "Prediction Rate",
y = "Count")+
theme_minimal()+
theme(legend.position = "none")
apply(preds_small_size_df_wide, 2, function(x) c(mean=mean(x), sd = sd(x)))
preds_boost <- as.numeric(predict(m1_boost, type = "response") >= 0.5)
contig_boost <- xtabs( ~ preds_boost + train_df$loan_status )
sum(diag(contig_boost))/ nrow(train_df)
preds_boost <- as.numeric(predict(m1_boost, type = "response") >= 0.5)
contig_boost <- xtabs( ~ preds_boost + train_df$loan_status )
sum(diag(contig_boost))/ nrow(train_df)
m1_boost
preds_RF <- m1_RF$confusion[,-3]
sum(diag(preds_RF))/ nrow(train_df)
set.seed(123)
m1_RF <- randomForest(loan_status ~ age + sex + education +
income + employment_experience +
home_ownership + loan_amount + loan_intent +
loan_int_rate + loan_percent_income +
credit_history_length + credit_score +
previous_loan_defaults_on_file,
data = train_df,
importance = TRUE)
preds_nb <- xtabs( ~ predict(m_nb_best, newdata = train_df) + train_df$loan_status)
sum(diag(preds_nb))/ nrow(train_df)
m_nb_best <- naiveBayes(loan_status ~ loan_percent_income +
loan_percent_income_sq +
loan_amount + loan_amount_sq +
loan_int_rate + loan_int_rate_sq +
home_ownership + loan_intent,
train_df)
plot(nb_m_select)
preds_qda <- xtabs( ~ predict(m_qda_best)$class + train_df$loan_status)
sum(diag(preds_qda))/ nrow(train_df)
plot(qda_m_select)
preds_lda <- xtabs( ~ predict(m_lda_best)$class + train_df$loan_status)
sum(diag(preds_lda))/ nrow(train_df)
plot(lda_m_select)
preds_ridge <- as.numeric(predict(m1_ridge,
s = m1_ridge$lambda,
newx = x_train, type = "response") >= 0.5)
preds_lasso <- as.numeric(predict(m1_lasso,
s = m1_lasso$lambda,
newx = x_train, type = "response") >= 0.5)
contig_ridge <- xtabs( ~ preds_ridge + train_df$loan_status)
contig_lasso <- xtabs( ~ preds_lasso + train_df$loan_status)
sum(diag(contig_ridge))/ nrow(train_df)
sum(diag(contig_lasso))/ nrow(train_df)
# Pseudo R^2
PseudoR2(m1_logistic_aic1, which = "Nagelkerke")
# Contingency table
# (for now use a basic threshold of 0.5)
preds.logistic <- as.numeric(m1_logistic_aic1$fitted.values >= .5)
cont.table.logistic <- table(train_df$loan_status, preds.logistic)
(sum(diag(cont.table.logistic))) / sum(cont.table.logistic)
preds_small_size
test.preds
length(test.preds)
set.seed(12345)
N_new_test_dfs <- 1000
small_test_df_size <- 250
preds_small_size <- matrix(ncol = length(test.preds), nrow = N_new_test_dfs)
for (test_df_N in 1:N_new_test_dfs){
# random 250 observations
rand.rows <- sample(1:nrow(test_df), small_test_df_size)
resp <- test_df$loan_status
# for each model
for (i in 1:length(test.preds)){
cont.table <- xtabs( ~ test.preds[[i]][rand.rows] + resp[rand.rows])
prop.cor <- sum(diag(cont.table))/ small_test_df_size
preds_small_size[test_df_N,i] <- prop.cor
}
# for random guessing
rang.guess.preds <- sample(c(0,1), 250, replace = T, prob = preds)
cont.table <- xtabs( ~ rang.guess.preds + resp[rand.rows])
prop.cor <- sum(diag(cont.table))/ small_test_df_size
preds_small_size[test_df_N,length(test.preds)+1] <- prop.cor
}
length(test.preds)
set.seed(12345)
N_new_test_dfs <- 1000
small_test_df_size <- 250
preds_small_size <- matrix(ncol = length(test.preds)+1, nrow = N_new_test_dfs)
for (test_df_N in 1:N_new_test_dfs){
# random 250 observations
rand.rows <- sample(1:nrow(test_df), small_test_df_size)
resp <- test_df$loan_status
# for each model
for (i in 1:length(test.preds)){
cont.table <- xtabs( ~ test.preds[[i]][rand.rows] + resp[rand.rows])
prop.cor <- sum(diag(cont.table))/ small_test_df_size
preds_small_size[test_df_N,i] <- prop.cor
}
# for random guessing
rang.guess.preds <- sample(c(0,1), 250, replace = T, prob = preds)
cont.table <- xtabs( ~ rang.guess.preds + resp[rand.rows])
prop.cor <- sum(diag(cont.table))/ small_test_df_size
preds_small_size[test_df_N,length(test.preds)+1] <- prop.cor
}
# Visualization
preds_small_size_df_wide <- as.data.frame(preds_small_size)
preds_small_size_df_wide
methods.preds
colnames(preds_small_size_df_wide) <- c(methods.preds, "Random")
